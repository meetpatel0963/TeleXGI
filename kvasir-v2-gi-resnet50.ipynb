{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6600393,"sourceType":"datasetVersion","datasetId":3808247},{"sourceId":6683244,"sourceType":"datasetVersion","datasetId":3855201},{"sourceId":7025723,"sourceType":"datasetVersion","datasetId":3925062},{"sourceId":7859446,"sourceType":"datasetVersion","datasetId":4609733}],"dockerImageVersionId":30559,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Installing the Required Libraries</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"!pip install timm grad-cam lime scikit-image scipy","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U kaleido","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Imports</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as transforms\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, WeightedRandomSampler, Dataset\nimport torchvision.models as models\nimport torch.optim as optim\n\nimport os\nimport time\nimport copy\nimport random\nimport pickle\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport cv2\nimport plotly.express as px\nimport plotly.io as pio\n\nfrom PIL import Image\n\nfrom collections import OrderedDict\nfrom typing import Tuple, Union\n\nfrom timm.scheduler import CosineLRScheduler\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchmetrics.classification import MulticlassAccuracy, MulticlassPrecision, \\\n    MulticlassRecall, MulticlassF1Score, MulticlassConfusionMatrix, \\\n    MulticlassMatthewsCorrCoef, MulticlassSpecificity","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Config Parameters</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class Config:\n    SEED = 2023\n    \n    # Model Parameters\n    RESOLUTION = 224\n    MLP_INPUT_DIM = 2048\n    MLP_HIDDEN_DIM = 512\n    MLP_DROPOUT_RATE = 0.2\n    \n    BATCH_SIZE = 64\n    NUM_WORKERS = 2\n    NUM_CLASSES = 8\n    \n    # Training Hyperparameters\n    COOLDOWN_EPOCHS = 5\n    TRAIN_EPOCHS = 25\n    NUM_EPOCHS = TRAIN_EPOCHS + COOLDOWN_EPOCHS\n    BASE_LR = 0.0001\n    WEIGHT_DECAY = 0.00001\n    PRINT_FREQ = 1024\n    SAVE_FREQ = 5\n    \n    # Cosine LR Decay Parameters\n    LR_MIN = 1e-6\n    WARMUP_LR_INIT = 5e-5\n    CYCLE_DECAY = 0.5\n    CYCLE_LIMIT = 1\n    WARMUP_EPOCHS = 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Paths</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"INPUT_DIR = '/kaggle/input'\nOUTPUT_DIR = '/kaggle/working/'\n\nBASE_DIR = INPUT_DIR + '/kvasir-v2/kvasir-dataset-v2'\nDATA_FILES_DIR = INPUT_DIR + '/kvasir-v2-folds'\n\nPRETRAINED_WEIGHTS_DIR = INPUT_DIR + '/kvasir-v2-resnet50-epochs-100'\nPRETRAINED_WEIGHTS_FILENAME = '/model_fold_0_epoch_30.pth'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATHS = [\n    '/dyed-lifted-polyps',\n    '/dyed-resection-margins',\n    '/esophagitis',\n    '/normal-cecum',\n    '/normal-pylorus',\n    '/polyps',\n    '/ulcerative-colitis',\n    '/normal-z-line',\n]\nIMAGES = [\n    '/008aa3ed-1812-4854-954c-120ae85bb6bd.jpg',\n    '/0062bbf3-58d7-435d-b0ca-381703c39911.jpg',\n    '/001fb927-4814-4ba5-851d-189db99291d8.jpg',\n    '/0ab26ff5-3161-4a17-bcf4-95663033af0a.jpg',\n    '/005959d0-b75b-41ed-8da1-2a5d0666d612.jpg',\n    '/00072d5f-7cd8-434c-8a5a-1a0bb2c9711d.jpg',\n    '/005b9962-41ed-4ae4-8aae-395bbab93fd7.jpg',\n    '/00bee375-36d2-4ba9-89e5-bd6132d79c0c.jpg'\n]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">EDA: Exploratory Data Analysis</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Function to load and display one image from each class\ndef display_sample_images(root_folder, num_classes=Config.NUM_CLASSES):\n    fig, axes = plt.subplots(2, 4, figsize=(12, 6), dpi=600)\n    axes = axes.flatten()\n\n    for i in range(num_classes):\n        img_path = root_folder + PATHS[i] + IMAGES[i]\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        axes[i].imshow(img)\n        axes[i].set_title(PATHS[i][1:])\n        axes[i].axis('off')\n\n    plt.tight_layout()\n    plt.savefig(OUTPUT_DIR + 'sample_images.pdf', bbox_inches='tight')\n    plt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_sample_images(BASE_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display data distribution across classes using plotly\ndef display_data_distribution(root_folder):\n    classes = os.listdir(root_folder)\n    num_images = [len(os.listdir(os.path.join(root_folder, class_folder))) for class_folder in classes]\n\n    df = pd.DataFrame({'Class': classes, 'Number of Images': num_images})\n    fig = px.bar(df, x='Class', y='Number of Images', title='Data Distribution Across Classes',\n                 labels={'Number of Images': 'Number of Images', 'Class': 'Class'},\n                 text='Number of Images',\n                 height=500)\n    fig.update_traces(texttemplate='%{text}', textposition='outside')\n    fig.update_layout(height=600, width=800, xaxis=dict(tickangle=45))\n    pio.write_image(fig, OUTPUT_DIR + 'data_distribution.pdf')\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_data_distribution(BASE_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display image size distribution for each class using plotly\ndef display_image_size_distribution(root_folder):\n    classes = os.listdir(root_folder)\n    data = []\n\n    for class_folder in classes:\n        class_path = os.path.join(root_folder, class_folder)\n        image_sizes = []\n\n        for img_file in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_file)\n            img = cv2.imread(img_path)\n            image_sizes.append(img.shape[:2])\n\n        image_sizes = np.array(image_sizes)\n        average_sizes = np.mean(image_sizes, axis=0)\n        data.append({'Class': class_folder, 'Width': average_sizes[0], 'Height': average_sizes[1]})\n\n    df = pd.DataFrame(data)\n\n    fig = px.bar(df, x='Class', y=['Width', 'Height'], title='Image Size Distribution Across Classes',\n                 labels={'value': 'Average Image Size (pixels)', 'variable': 'Dimension'},\n                 color_discrete_sequence=['skyblue', 'salmon'])\n\n    # Add text annotations inside each bar\n    for i, class_label in enumerate(df['Class']):\n        fig.add_annotation(\n            x=class_label,\n            y=df.loc[i, 'Width'] / 2,\n            text=f'{round(df.loc[i, \"Width\"], 1)}',\n            showarrow=False,\n            font=dict(color='white', size=14)\n        )\n        fig.add_annotation(\n            x=class_label,\n            y=df.loc[i, \"Width\"] + (df.loc[i, 'Height'] / 2),\n            text=f'{round(df.loc[i, \"Height\"], 1)}',\n            showarrow=False,\n            font=dict(color='white', size=14)\n        )\n\n    # Update layout\n    fig.update_layout(xaxis=dict(tickangle=45))\n    pio.write_image(fig, OUTPUT_DIR + 'image_size_distribution.pdf')\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_image_size_distribution(BASE_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to display color distribution using plotly\ndef display_color_distribution(root_folder):\n    classes = os.listdir(root_folder)\n    data = []\n\n    for class_folder in classes:\n        class_path = os.path.join(root_folder, class_folder)\n        class_color_distribution = np.zeros(3)\n\n        for img_file in os.listdir(class_path):\n            img_path = os.path.join(class_path, img_file)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            class_color_distribution += np.mean(img, axis=(0, 1))\n\n        class_color_distribution /= len(os.listdir(class_path))\n        data.append({'Class': class_folder, 'Red': class_color_distribution[0],\n                     'Green': class_color_distribution[1], 'Blue': class_color_distribution[2]})\n\n    df = pd.DataFrame(data)\n\n    fig = px.bar(df, x='Class', y=['Red', 'Green', 'Blue'], title='Color Distribution Across Classes',\n                 labels={'value': 'Average Color Value', 'variable': 'Color Channel'},\n                 color_discrete_sequence=['red', 'green', 'blue'])\n\n    # Add text annotations inside each color bar\n    for i, class_label in enumerate(df['Class']):\n        fig.add_annotation(\n            x=class_label,\n            y=df.loc[i, 'Red'] / 2,\n            text=f'{round(df.loc[i, \"Red\"], 1)}',\n            showarrow=False,\n            font=dict(color='white', size=14)\n        )\n        fig.add_annotation(\n            x=class_label,\n            y=df.loc[i, 'Red'] + (df.loc[i, 'Green'] / 2),\n            text=f'{round(df.loc[i, \"Green\"], 1)}',\n            showarrow=False,\n            font=dict(color='white', size=14)\n        )\n        fig.add_annotation(\n            x=class_label,\n            y=df.loc[i, 'Red'] + df.loc[i, 'Green'] + (df.loc[i, 'Blue'] / 2),\n            text=f'{round(df.loc[i, \"Blue\"], 1)}',\n            showarrow=False,\n            font=dict(color='white', size=14)\n        )\n\n    # Update layout\n    fig.update_layout(xaxis=dict(tickangle=45))\n    pio.write_image(fig, OUTPUT_DIR + 'color_distribution.pdf')\n    fig.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_color_distribution(BASE_DIR)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kvasir_v2_dataset = ImageFolder(BASE_DIR)\n\n# Extract class names\nclass_names = kvasir_v2_dataset.classes\n\n# Count the number of images in each class\nclass_counts = [len(os.listdir(os.path.join(BASE_DIR, class_name))) for class_name in class_names]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kvasir_v2_dataset = datasets.ImageFolder(BASE_DIR, transform=transforms.ToTensor())\n\n# Compute mean and std\nmean = 0.0\nfor dataset in [kvasir_v2_dataset]:\n    for images, _ in dataset:\n        mean += images.mean([1,2])\nmean /= len(kvasir_v2_dataset)\n\nstd = 0.0\nfor dataset in [kvasir_v2_dataset]:\n    for images, _ in dataset:\n        std += ((images - mean.unsqueeze(1).unsqueeze(2))**2).mean([1,2])\nstd = torch.sqrt(std / len(kvasir_v2_dataset))\n\nprint(mean, std)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Defining Data Transforms</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"normalize = transforms.Normalize(mean=[0.4857, 0.3460, 0.2983],\n                                 std=[0.3348, 0.2456, 0.2369])\n\n# Define data augmentation transformations\ntrain_transforms = transforms.Compose([\n    transforms.Resize((Config.RESOLUTION, Config.RESOLUTION)),\n    transforms.RandomApply([transforms.RandomHorizontalFlip()], p=0.5),\n    transforms.RandomApply([transforms.RandomVerticalFlip()], p=0.5),\n    transforms.RandomApply([transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)], p=0.5),\n    transforms.RandomApply([transforms.RandomResizedCrop(Config.RESOLUTION, scale=(0.8, 1.0))], p=0.5),\n    transforms.RandomApply([transforms.GaussianBlur(kernel_size=7)], p=0.5),\n    transforms.RandomApply([transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.9, 1.1))], p=0.5),\n    transforms.RandomApply([transforms.RandomAffine(degrees=45)], p=0.5),\n    transforms.RandomApply([transforms.RandomPerspective(distortion_scale=0.5)], p=0.5),\n    transforms.RandomApply([transforms.RandomAffine(degrees=10, shear=10)], p=0.5),\n    transforms.ToTensor(),\n    normalize,\n])\n\n# Validation transformations without augmentation\nvalid_transforms = transforms.Compose([\n    transforms.Resize((Config.RESOLUTION, Config.RESOLUTION)),\n    transforms.ToTensor(),\n    normalize,\n])\n\ntest_transforms = transforms.Compose([\n    transforms.Resize((Config.RESOLUTION, Config.RESOLUTION)),\n    transforms.ToTensor(),\n    normalize,\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Creating K Folds</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"file_paths = []\nlabels = []\n\n# Iterate through each category folder\nfor category in os.listdir(BASE_DIR):\n    category_folder = os.path.join(BASE_DIR, category)\n    \n    if os.path.isdir(category_folder):\n        # List all image files in the category folder\n        image_files = [os.path.join(category_folder, file) for file in os.listdir(category_folder) if file.endswith('.jpg')]\n        \n        # Add file paths and labels to the respective lists\n        file_paths.extend(image_files)\n        labels.extend([category] * len(image_files))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(file_paths))\nprint(len(labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Set the number of folds for k-fold cross-validation\n# k_folds = 5\n\n# # Create a k-fold cross-validation splitter\n# kf = StratifiedKFold(n_splits=k_folds, shuffle=True, random_state=Config.SEED)\n\n# # Iterate over the k-folds\n# for fold_idx, (train_indices, val_test_indices) in enumerate(kf.split(file_paths, labels)):\n#     num_val_test = len(val_test_indices)\n#     num_val = int(0.5 * num_val_test)\n#     num_test = num_val_test - num_val\n    \n#     # Initialize empty lists for validation and test indices\n#     val_indices = []\n#     test_indices = []\n    \n#     # Count the number of samples for each class\n#     class_counts = {label: 0 for label in set(labels)}\n#     for idx in val_test_indices:\n#         label = labels[idx]\n#         class_counts[label] += 1\n    \n#     test_class_counts = {label: 0 for label in set(labels)}\n    \n#     for idx in val_test_indices:\n#         label = labels[idx]\n#         if test_class_counts[label] < class_counts[label] // 2:\n#             test_indices.append(idx)\n#             test_class_counts[label] += 1\n#         else:\n#             val_indices.append(idx)\n    \n#     train_files = [file_paths[i] for i in train_indices]\n#     valid_files = [file_paths[i] for i in val_indices]\n#     test_files = [file_paths[i] for i in test_indices]\n    \n#     with open(OUTPUT_DIR + f'/data_files_fold_{fold_idx}.pkl', 'wb') as f:\n#         pickle.dump((train_files, valid_files, test_files), f)\n    \n#     print(len(train_indices))\n#     print(len(val_indices))\n#     print(len(test_indices))\n    \n#     print(len(train_files))\n#     print(len(valid_files))\n#     print(len(test_files))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Populate Data Loaders</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Weighted Random Sampling: To balance the dataset\ndef build_sampler(train_dataset):\n    y_train = [label for (path, label) in train_dataset.samples]\n\n    class_sample_count = np.array(\n        [len(np.where(y_train == t)[0]) for t in np.unique(y_train)])\n\n    weight = 1. / class_sample_count\n    samples_weight = np.array([weight[t] for t in y_train])\n    samples_weight = torch.from_numpy(samples_weight)\n\n    sampler = WeightedRandomSampler(samples_weight.type('torch.DoubleTensor'), len(samples_weight))\n    return sampler\n\n\ndef create_data_loader(dataset, batch_size, shuffle=True, num_workers=Config.NUM_WORKERS, \\\n                        pin_memory=True, sampler=None):\n    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers, \\\n                      pin_memory=pin_memory, sampler=sampler) ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k_folds = 5\nTRAIN_DATALOADERS = []\nVALID_DATALOADERS = []\nTEST_DATALOADERS = []","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for fold_idx in range(k_folds):\n    with open(DATA_FILES_DIR + f'/data_files_fold_{fold_idx}.pkl', 'rb') as f:\n        train_files, valid_files, test_files = pickle.load(f)\n    \n    # Initialize and load the train and validation datasets using ImageFolder\n    train_dataset = ImageFolder(root=BASE_DIR, transform=train_transforms)\n    valid_dataset = ImageFolder(root=BASE_DIR, transform=valid_transforms)\n    test_dataset = ImageFolder(root=BASE_DIR, transform=test_transforms)\n\n    # Filter train and validation datasets to include only the selected files\n    train_dataset.samples = [(path, label) for path, label in train_dataset.samples if path in train_files]\n    valid_dataset.samples = [(path, label) for path, label in valid_dataset.samples if path in valid_files]\n    test_dataset.samples = [(path, label) for path, label in test_dataset.samples if path in test_files]\n\n    # print(len(train_dataset))\n    # print(len(valid_dataset))\n    # print(len(test_dataset))\n\n    sampler = build_sampler(train_dataset)\n    train_dataloader = create_data_loader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=(sampler is None), num_workers=Config.NUM_WORKERS, pin_memory=True, sampler=sampler)\n    valid_dataloader = create_data_loader(valid_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,num_workers=Config.NUM_WORKERS, pin_memory=True)\n    test_dataloader = create_data_loader(test_dataset, batch_size=Config.BATCH_SIZE, shuffle=False,num_workers=Config.NUM_WORKERS, pin_memory=True)\n\n    TRAIN_DATALOADERS.append(train_dataloader)\n    VALID_DATALOADERS.append(valid_dataloader)\n    TEST_DATALOADERS.append(test_dataloader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Defining ResNet-50 Classifier</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"class ResNet50Classifier(nn.Module):\n    \n    def __init__(self, mlp_input_dim=Config.MLP_INPUT_DIM, mlp_hidden_dim=Config.MLP_HIDDEN_DIM, \\\n                 mlp_dropout_rate=Config.MLP_DROPOUT_RATE, num_classes=Config.NUM_CLASSES):\n        \n        super(ResNet50Classifier, self).__init__()\n        \n        # Load the pre-trained ResNet-50 model\n        self.resnet50 = models.resnet50(pretrained=True)\n        \n        # Remove the final classification layer\n        self.resnet50 = nn.Sequential(*list(self.resnet50.children())[:-1])\n        \n        # Define an MLP for classification\n        self.fc = nn.Sequential(\n            nn.Linear(mlp_input_dim, mlp_hidden_dim),\n            nn.ReLU(),\n            nn.Linear(mlp_hidden_dim, num_classes)\n        )\n    \n    def forward(self, x):\n        # Forward pass through ResNet-50\n        resnet_features = self.resnet50(x)\n        \n        # Flatten the features\n        resnet_features = resnet_features.view(resnet_features.size(0), -1)\n        \n        # Forward pass through MLP\n        output = self.fc(resnet_features)\n        return output","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"use_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = ResNet50Classifier().to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Plot Cosine Annealing Learning Rate Schedule</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"NUM_STEPS_PER_EPOCH = len(train_dataset) // Config.BATCH_SIZE\n\ndef plot_lrs_for_timm_scheduler(scheduler):\n    lrs = []\n\n    for epoch in range(Config.NUM_EPOCHS):\n        num_updates = epoch * NUM_STEPS_PER_EPOCH\n\n        for i in range(NUM_STEPS_PER_EPOCH):\n            num_updates += 1\n            scheduler.step_update(num_updates=num_updates)\n\n        scheduler.step(epoch + 1)\n\n        lrs.append(optimizer.param_groups[0][\"lr\"])\n    return lrs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Define optimizer\noptimizer = optim.AdamW(model.parameters(), lr=Config.BASE_LR, weight_decay=Config.WEIGHT_DECAY)\n\n# Cosine Annealing Learning Rate Scheduler\nscheduler = CosineLRScheduler(optimizer, t_initial=Config.TRAIN_EPOCHS, cycle_decay=Config.CYCLE_DECAY, lr_min=Config.LR_MIN,\n                              t_in_epochs=True, warmup_t=Config.WARMUP_EPOCHS, warmup_lr_init=Config.WARMUP_LR_INIT, \n                              cycle_limit=Config.CYCLE_LIMIT, warmup_prefix=True)\n\n\nlrs = plot_lrs_for_timm_scheduler(scheduler)\n\nplt.plot(range(1, Config.NUM_EPOCHS+1), lrs)\n\nplt.xlabel('Epochs')\nplt.ylabel('Learning Rate')\nplt.xticks(np.arange(0, Config.NUM_EPOCHS, 5))\n\nplt.grid(True, which='major', color='#666666', linestyle='-', alpha=0.4)\nplt.minorticks_on()\nplt.grid(True, which='minor', color='#999999', linestyle='-', alpha=0.2)\n\nplt.savefig(OUTPUT_DIR + 'learning_rate.pdf', dpi=600, bbox_inches='tight')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Defining Train, Validation, and Test Functions</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"# Training loop\ndef train_model(epoch, model, train_dataloader, device, metrics_data, metrics_keys):\n    print()\n    print('*****Train*****')\n    \n    model.train()\n    total_loss = 0.0\n    total_samples = 0\n    \n    num_steps_per_epoch = len(train_dataloader)\n    num_updates = epoch * num_steps_per_epoch\n\n    for i, (inputs, labels) in enumerate(train_dataloader):\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        loss = criterion(outputs, labels)\n        \n        # Backward pass and optimization\n        loss.backward()\n        optimizer.step()\n        scheduler.step_update(num_updates=num_updates)\n        \n        total_loss += loss.item() * inputs.size(0)\n        total_samples += inputs.size(0)\n        \n        # Update metrics\n        for metric in metrics_keys:\n            metrics_data['train'][metric].update(outputs, labels)\n        \n        if total_samples % Config.PRINT_FREQ == 0:\n            print(f'images count: {total_samples}')\n        \n    scheduler.step(epoch + 1)\n    \n    # Calculate average loss\n    avg_loss = total_loss / total_samples\n    \n    cur_metrics = dict()\n    for metric in metrics_keys:\n        cur_metrics[metric] = metrics_data['train'][metric].compute()\n    \n    return avg_loss, cur_metrics\n\n\n# Validation loop\ndef validate_model(epoch, model, val_loader, device, best_val_acc, best_model_wts, metrics_data, metrics_keys):\n    print()\n    print('*****Validation*****')\n\n    model.eval()\n    total_loss = 0.0\n    correct_predictions = 0\n    total_samples = 0\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(val_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            preds = model(inputs)\n            \n            # Compute loss\n            loss = criterion(preds, labels)\n\n            total_loss += loss.item() * inputs.size(0)\n            total_samples += inputs.size(0)\n            \n            # Update metrics\n            for metric in metrics_keys:\n                metrics_data['valid'][metric].update(preds, labels)\n\n            if total_samples % Config.PRINT_FREQ == 0:\n                print(f'images count: {total_samples}')\n    \n    # Calculate average loss\n    avg_loss = total_loss / total_samples\n    \n    cur_metrics = dict()\n    for metric in metrics_keys:\n        cur_metrics[metric] = metrics_data['valid'][metric].compute()\n    \n    accuracy = cur_metrics['acc']\n    if accuracy > best_val_acc:\n        best_val_acc = accuracy\n        best_model_wts = copy.deepcopy(model.state_dict())\n\n    return avg_loss, cur_metrics\n\n\n# Test loop\ndef test_model(model, test_dataloader, device, fold, metrics_data, metrics_keys):\n    print()\n    print('*****Test*****')\n    \n    model.eval()\n    since = time.time()\n    total_samples = 0\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(test_dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            # Forward pass\n            preds = model(inputs)\n            \n            total_samples += inputs.size(0)\n            \n            for metric in metrics_keys:\n                metrics_data['test'][metric].update(preds, labels)\n\n            if total_samples % Config.PRINT_FREQ == 0:\n                print(f'images count: {total_samples}')\n    \n    test_metrics = dict()\n    for metric in metrics_keys:\n        test_metrics[metric] = metrics_data['test'][metric].compute()\n        \n    torch.save(test_metrics, OUTPUT_DIR + f'/kvasir-v2-fold_{fold}.pt')\n    \n    print('Test Metrics: ')\n    print(test_metrics)\n    \n    time_elapsed = time.time() - since\n    print('Testing complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Perform Training</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"since = time.time()\n\nfor fold in range(1):\n    print()\n    print(f'---------------------------Fold {fold}---------------------------')\n    \n    model = ResNet50Classifier().to(device)\n    \n    total_params = sum(p.numel() for p in model.parameters())\n    print(f\"{total_params:,} total parameters.\")\n    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    total_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n    print(f\"{total_trainable_params:,} training parameters.\")\n    \n#     MODEL_PATH = PRETRAINED_WEIGHTS_DIR + PRETRAINED_WEIGHTS_FILENAME\n#     checkpoint = torch.load(MODEL_PATH, map_location=device)\n#     model.load_state_dict(checkpoint)\n    \n    best_model_wts = model.state_dict()\n    best_val_acc = 0.0\n\n    metrics_data = dict()\n    metrics_keys = ['acc', 'precision', 'micro-precision', 'recall', 'f1score', 'specificity', 'mcc', 'confusion_mat']\n    for phase in ['train', 'valid', 'test']:\n        metrics_data[phase] = {\n            'acc': MulticlassAccuracy(num_classes=Config.NUM_CLASSES).to(device),\n            'precision': MulticlassPrecision(num_classes=Config.NUM_CLASSES).to(device),\n            'micro-precision':  MulticlassPrecision(num_classes=Config.NUM_CLASSES, average='micro').to(device),\n            'recall': MulticlassRecall(num_classes=Config.NUM_CLASSES).to(device),\n            'f1score': MulticlassF1Score(num_classes=Config.NUM_CLASSES).to(device),\n            'specificity': MulticlassSpecificity(num_classes=Config.NUM_CLASSES).to(device),\n            'mcc': MulticlassMatthewsCorrCoef(num_classes=Config.NUM_CLASSES).to(device),\n            'confusion_mat': MulticlassConfusionMatrix(num_classes=Config.NUM_CLASSES).to(device)\n        }\n\n\n    # Define loss function\n    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n\n    # Define optimizer\n    optimizer = optim.AdamW(model.parameters(), lr=Config.BASE_LR, weight_decay=Config.WEIGHT_DECAY)\n\n    # Cosine Annealing Learning Rate Scheduler\n    scheduler = CosineLRScheduler(optimizer, t_initial=Config.TRAIN_EPOCHS, cycle_decay=Config.CYCLE_DECAY, lr_min=Config.LR_MIN,\n                                  t_in_epochs=True, warmup_t=Config.WARMUP_EPOCHS, warmup_lr_init=Config.WARMUP_LR_INIT, \n                                  cycle_limit=Config.CYCLE_LIMIT, warmup_prefix=True)\n\n    \n    for epoch in range(Config.NUM_EPOCHS):\n        train_loader = TRAIN_DATALOADERS[fold]\n        validation_loader = VALID_DATALOADERS[fold]\n        \n        # Train the model\n        train_loss, train_metrics = train_model(\n            epoch, model, train_loader, device, metrics_data, metrics_keys\n        )\n\n        print(\n            f\"Epoch {epoch + 1}/{Config.NUM_EPOCHS} - \"\n            f\"LR: {optimizer.state_dict()['param_groups'][0]['lr']}, \"\n            f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_metrics['acc']:.4f}, Train Mirco-P: {train_metrics['micro-precision']:.4f}, Train MCC: {train_metrics['mcc']:.4f}\"\n        )\n\n        # Validate the model\n        val_loss, val_metrics = validate_model(epoch, model, validation_loader, device, best_val_acc, best_model_wts, metrics_data, metrics_keys)\n\n        # Print epoch information\n        print(\n            f\"Epoch {epoch + 1}/{Config.NUM_EPOCHS} - \"\n            f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_metrics['acc']:.4f}, Val Mirco-P: {val_metrics['micro-precision']:.4f}, Val MCC: {val_metrics['mcc']:.4f}\"\n        )\n\n        time_elapsed = time.time() - since\n        print('Epoch {} complete in {:.0f}m {:.0f}s'.format(epoch+1, time_elapsed // 60, time_elapsed % 60))\n\n        epoch_data = {\n            'epoch': epoch+1,\n            'lr': optimizer.state_dict()['param_groups'][0]['lr'],\n            'train_loss': train_loss,\n            'train_metrics': train_metrics,\n            'valid_loss': val_loss, \n            'valid_metrics': val_metrics\n        }\n\n        if (epoch+1) % Config.SAVE_FREQ == 0:\n            epoch_data['model_state_dict'] = model.state_dict()\n            epoch_data['optimizer_state_dict'] = optimizer.state_dict()\n            epoch_data['scheduler_state_dict'] = scheduler.state_dict()\n\n        torch.save(epoch_data, OUTPUT_DIR + '/model_fold_{}_epoch_{}.pth'.format(fold, epoch+1))\n        print()\n    \n    time_elapsed = time.time() - since\n    print('Training Complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best Validation Accuracy: {:.4f}'.format(best_val_acc))\n\n    torch.save(best_model_wts, OUTPUT_DIR + f'/best_model_fold_{fold}.pth')\n    model.load_state_dict(best_model_wts)\n    \n    test_dataloader = TEST_DATALOADERS[fold]\n    test_model(model, test_dataloader, device, fold, metrics_data, metrics_keys)\n    \n    print(f'-----------------------------------------------------------------')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<div style=\"background-color: #34495e; border-bottom: 5px solid #95a5a6; padding: 10px;\">\n    <h2 style=\"color: white;\">Perform Testing</h2>\n</div>","metadata":{}},{"cell_type":"code","source":"model = ResNet50Classifier().to(device)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"{total_params:,} total parameters.\")\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\ntotal_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"{total_trainable_params:,} training parameters.\")\n\nMODEL_PATH = PRETRAINED_WEIGHTS_DIR + PRETRAINED_WEIGHTS_FILENAME\ncheckpoint = torch.load(MODEL_PATH, map_location=device)\nmodel.load_state_dict(checkpoint['model_state_dict'])\n\nmetrics_data = dict()\nmetrics_keys = ['acc', 'precision', 'micro-precision', 'recall', 'f1score', 'specificity', 'mcc', 'confusion_mat']\nfor phase in ['test']:\n    metrics_data[phase] = {\n        'acc': MulticlassAccuracy(num_classes=Config.NUM_CLASSES).to(device),\n        'precision': MulticlassPrecision(num_classes=Config.NUM_CLASSES).to(device),\n        'micro-precision':  MulticlassPrecision(num_classes=Config.NUM_CLASSES, average='micro').to(device),\n        'recall': MulticlassRecall(num_classes=Config.NUM_CLASSES).to(device),\n        'f1score': MulticlassF1Score(num_classes=Config.NUM_CLASSES).to(device),\n        'specificity': MulticlassSpecificity(num_classes=Config.NUM_CLASSES).to(device),\n        'mcc': MulticlassMatthewsCorrCoef(num_classes=Config.NUM_CLASSES).to(device),\n        'confusion_mat': MulticlassConfusionMatrix(num_classes=Config.NUM_CLASSES).to(device)\n    }\n\n\nfold = 0\ntest_dataloader = TEST_DATALOADERS[fold]\ntest_model(model, test_dataloader, device, fold, metrics_data, metrics_keys)","metadata":{},"execution_count":null,"outputs":[]}]}